---
phase: 18-safety-rails
plan: 02
type: execute
wave: 2
depends_on: ["18-01"]
files_modified:
  - app/(chat)/api/chat/route.ts
  - lib/types.ts
  - components/chat.tsx
  - lib/bot-personalities.ts
  - lib/ai/tools/request-suggestions.ts
autonomous: true

must_haves:
  truths:
    - "When AI response hits maxOutputTokens, user sees a truncation indicator with option to continue"
    - "AI suggests contacting human support when it cannot adequately help"
    - "Follow-up suggestions are validated for length limits and PII content on the server"
  artifacts:
    - path: "app/(chat)/api/chat/route.ts"
      provides: "Truncation detection via finishReason === 'length' emitting data-truncated event"
    - path: "lib/types.ts"
      provides: "truncated: boolean added to CustomUIDataTypes"
    - path: "components/chat.tsx"
      provides: "Truncation state handling in onData and reset on new message"
    - path: "lib/bot-personalities.ts"
      provides: "Human escalation instructions in IDENTITY_RULES"
    - path: "lib/ai/tools/request-suggestions.ts"
      provides: "Server-side suggestion length + PII validation"
  key_links:
    - from: "app/(chat)/api/chat/route.ts"
      to: "components/chat.tsx"
      via: "data-truncated event through dataStream"
      pattern: "data-truncated"
    - from: "lib/bot-personalities.ts"
      to: "lib/ai/prompts.ts"
      via: "getSystemPrompt includes IDENTITY_RULES with escalation"
      pattern: "HUMAN SUPPORT ESCALATION"
    - from: "lib/safety/pii-redactor.ts"
      to: "lib/ai/tools/request-suggestions.ts"
      via: "import redactPII for suggestion content"
      pattern: "redactPII.*suggestedText"
---

<objective>
Add truncation detection with client-side indicator, human escalation instructions in AI system prompt, and server-side suggestion content validation with PII checks.

Purpose: Addresses SAFE-04 (human escalation), SAFE-05 (truncation indicator), and SAFE-06 (suggestion validation). These are the user-facing safety features that depend on the server-side infrastructure from Plan 01.

Output: Truncation detection in chat route with UI feedback, human escalation instructions in system prompt, and hardened suggestion validation in the request-suggestions tool.
</objective>

<execution_context>
@/home/qualia/.claude/get-shit-done/workflows/execute-plan.md
@/home/qualia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/18-safety-rails/18-RESEARCH.md
@.planning/phases/18-safety-rails/18-01-SUMMARY.md

@app/(chat)/api/chat/route.ts
@components/chat.tsx
@lib/types.ts
@lib/bot-personalities.ts
@lib/ai/tools/request-suggestions.ts
@lib/ai/parse-suggestions.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Truncation detection in chat route and client-side handling</name>
  <files>app/(chat)/api/chat/route.ts, lib/types.ts, components/chat.tsx</files>
  <action>
**app/(chat)/api/chat/route.ts -- Truncation event (SAFE-05):**
- In the `streamText`'s `onFinish` callback (the inner one that receives `{ usage }`), add `finishReason` to the destructured params: `onFinish: async ({ usage, finishReason }) => { ... }`
- After the existing usage tracking logic (the `dataStream.write({ type: "data-usage", data: finalMergedUsage })` block), add truncation detection:
  ```
  if (finishReason === "length") {
    dataStream.write({ type: "data-truncated", data: true });
    logger.info({ chatId: id, maxOutputTokens: isSimple ? 500 : 4096 }, "AI response truncated due to maxOutputTokens");
  }
  ```
- The `data-truncated` event uses the same `dataStream.write()` pattern already used for `data-usage`.

**lib/types.ts -- Add truncated type:**
- In the `CustomUIDataTypes` type, add `truncated: boolean;` to the interface. This enables typed handling of the `data-truncated` event in the client.

**components/chat.tsx -- Handle truncation state:**
- Add state: `const [isTruncated, setIsTruncated] = useState(false);`
- In the `onData` callback of `useChat`, add handling for truncation events:
  ```
  if (dataPart.type === "data-truncated") {
    setIsTruncated(true);
  }
  ```
- In the `onFinish` callback, reset truncation: add `setIsTruncated(false);` (it only stays true for the duration of the current message display)
- In the `sendMessage` wrapper function, reset truncation when user sends a new message: add `setIsTruncated(false);` at the start.
- Pass `isTruncated` to the `Messages` component as a prop. If `Messages` doesn't accept it yet, that's fine -- just add it as a prop being passed. The `Messages` component will display a simple inline notice below the last assistant message when `isTruncated` is true.
- Add a simple truncation notice inside the message area (after the `<Messages />` component, still inside the messages container div). When `isTruncated && status === "ready"`:
  ```tsx
  {isTruncated && status === "ready" && (
    <div className="mx-auto max-w-3xl px-4 pb-2">
      <div className="flex items-center gap-2 rounded-lg border border-amber-200/50 bg-amber-50/50 px-3 py-2 text-sm text-amber-800 dark:border-amber-800/50 dark:bg-amber-950/30 dark:text-amber-200">
        <span>This response was truncated due to length limits.</span>
        <button
          type="button"
          className="ml-auto shrink-0 font-medium underline underline-offset-2 hover:no-underline"
          onClick={() => {
            setInput("Please continue your previous response from where you left off.");
            setIsTruncated(false);
          }}
        >
          Continue
        </button>
      </div>
    </div>
  )}
  ```
  Place this AFTER the `<Messages />` component but inside the same flex container (the `div` with `className="flex h-full w-full flex-col overflow-hidden"`), before the closing `</div>`. This way it appears below messages but above the input area.
  </action>
  <verify>
Run `pnpm lint` to confirm no type errors. Verify:
1. `chat/route.ts` emits `data-truncated` event when `finishReason === "length"`
2. `lib/types.ts` includes `truncated: boolean` in `CustomUIDataTypes`
3. `components/chat.tsx` handles truncation state with proper reset on new message and onFinish
4. Truncation banner renders with "Continue" button that prefills input
  </verify>
  <done>When AI response hits maxOutputTokens (finishReason === "length"), a `data-truncated` event is emitted, client shows an amber banner below the message with "Continue" button that prefills a continuation prompt, and state resets correctly on new messages.</done>
</task>

<task type="auto">
  <name>Task 2: Human escalation prompt and server-side suggestion validation</name>
  <files>lib/bot-personalities.ts, lib/ai/tools/request-suggestions.ts</files>
  <action>
**lib/bot-personalities.ts -- Human escalation instructions (SAFE-04):**
- Add a `HUMAN_ESCALATION_INSTRUCTIONS` constant (same pattern as `FORMATTING_INSTRUCTIONS`, `CONTENT_GENERATION_INSTRUCTIONS`, `IDENTITY_RULES`):
  ```
  const HUMAN_ESCALATION_INSTRUCTIONS = `
  ## HUMAN SUPPORT ESCALATION
  When you encounter any of these situations, proactively suggest contacting the human support team:
  1. You have been unable to answer the same question after 2 or more attempts
  2. The user explicitly asks for human help or to speak with a person
  3. The question is about billing, account issues, or technical problems you cannot solve
  4. The user expresses frustration with your responses

  **How to escalate:**
  "I want to make sure you get the best help possible. You can reach our support team directly through the support widget (the chat icon in the toolbar) or email us at support@aleccimedia.com."

  NEVER refuse to try helping first. Always attempt a response, but suggest support as an additional option when appropriate.`;
  ```
- Append `${HUMAN_ESCALATION_INSTRUCTIONS}` to each of the three system prompts in `SYSTEM_PROMPTS` (alexandria, kim, collaborative). Add it after the existing KNOWLEDGE BASE OWNERSHIP section at the end of each prompt template.
- Use the same interpolation pattern as `${FORMATTING_INSTRUCTIONS}` and `${CONTENT_GENERATION_INSTRUCTIONS}`.

**lib/ai/tools/request-suggestions.ts -- Server-side suggestion validation (SAFE-06):**
- Import `{ redactPII }` from `@/lib/safety/pii-redactor`
- In the `for await (const element of elementStream)` loop, BEFORE creating the `suggestion` object, apply validation:
  1. Length limits: truncate `originalSentence` and `suggestedSentence` to 500 chars, `description` to 200 chars using `.slice(0, N)`
  2. PII redaction: run `redactPII()` on each truncated string to strip any PII that leaked into suggestions
  3. Default to empty string if any field is undefined/null: `(element.originalSentence || "").slice(0, 500)`
- Construct the suggestion object using the validated/redacted values:
  ```
  const originalText = redactPII((element.originalSentence || "").slice(0, 500)).text;
  const suggestedText = redactPII((element.suggestedSentence || "").slice(0, 500)).text;
  const description = redactPII((element.description || "").slice(0, 200)).text;

  const suggestion: SuggestionDraft = {
    originalText,
    suggestedText,
    description,
    id: generateUUID(),
    documentId,
    isResolved: false,
  };
  ```
- This complements the existing client-side 100-char truncation in `parse-suggestions.ts` by adding server-side enforcement with PII safety. The client-side parser handles inline suggestions from the AI response text; this handles the `requestSuggestions` tool output.
  </action>
  <verify>
Run `pnpm lint` to confirm no import or type errors. Verify:
1. `lib/bot-personalities.ts` includes HUMAN_ESCALATION_INSTRUCTIONS in all three executive prompts
2. `lib/ai/tools/request-suggestions.ts` imports `redactPII` and applies length + PII validation to all suggestion fields
3. Escalation text mentions "support widget" and "support@aleccimedia.com"
4. Suggestion fields are truncated to 500/500/200 chars respectively
  </verify>
  <done>AI system prompt includes human escalation instructions that trigger on repeated failures, explicit requests, billing/account questions, and user frustration. Suggestion tool validates content with length limits and PII redaction on the server before sending to client or saving to database.</done>
</task>

</tasks>

<verification>
1. `finishReason === "length"` emits `data-truncated` event to client
2. Client shows truncation banner with "Continue" button below truncated messages
3. Truncation state resets when user sends new message
4. All three executive system prompts include human escalation instructions
5. Escalation prompt is specific enough to avoid over-eager support suggestions
6. `requestSuggestions` tool validates suggestion length (500/500/200) and redacts PII
7. `pnpm lint` passes with no errors
</verification>

<success_criteria>
- When AI maxOutputTokens is hit, user sees amber "Response was truncated" banner with Continue button
- Continue button prefills "Please continue your previous response from where you left off"
- AI suggests human support when unable to help after 2+ attempts, on explicit request, or for billing/account issues
- Suggestion tool output has length limits enforced and PII stripped before reaching client
- `pnpm lint` clean
</success_criteria>

<output>
After completion, create `.planning/phases/18-safety-rails/18-02-SUMMARY.md`
</output>
