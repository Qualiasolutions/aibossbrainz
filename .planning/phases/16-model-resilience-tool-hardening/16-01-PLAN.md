---
phase: 16-model-resilience-tool-hardening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/ai/providers.ts
  - app/(chat)/actions.ts
  - lib/ai/conversation-summarizer.ts
  - app/(chat)/api/chat/route.ts
autonomous: true

must_haves:
  truths:
    - "When the primary AI model is unavailable, chat automatically falls back to a secondary model and the user gets a response"
    - "The AI model identifier in providers.ts is a stable versioned ID, not a preview/unstable slug"
    - "Title generation cannot hang indefinitely -- it times out after 10s and returns a fallback title"
    - "Conversation summary generation cannot hang indefinitely -- it times out after 10s and returns null gracefully"
    - "The main streamText call has a timeout that prevents Vercel 504s and detects stalled streams"
  artifacts:
    - path: "lib/ai/providers.ts"
      provides: "Stable model IDs with OpenRouter fallback chain"
      contains: "google/gemini-2.5-flash"
    - path: "app/(chat)/actions.ts"
      provides: "Title generation with resilience wrapper and timeout"
      contains: "withAIGatewayResilience"
    - path: "lib/ai/conversation-summarizer.ts"
      provides: "Summary generation with resilience wrapper and timeout"
      contains: "withAIGatewayResilience"
    - path: "app/(chat)/api/chat/route.ts"
      provides: "streamText with timeout configuration"
      contains: "timeout"
  key_links:
    - from: "lib/ai/providers.ts"
      to: "@openrouter/ai-sdk-provider"
      via: "extraBody.models array for fallback"
      pattern: "extraBody.*models"
    - from: "app/(chat)/actions.ts"
      to: "lib/resilience.ts"
      via: "withAIGatewayResilience wrapper"
      pattern: "withAIGatewayResilience"
    - from: "lib/ai/conversation-summarizer.ts"
      to: "lib/resilience.ts"
      via: "withAIGatewayResilience wrapper"
      pattern: "withAIGatewayResilience"
---

<objective>
Harden the AI model layer so chat survives model outages and background AI tasks (title generation, conversation summaries) cannot hang indefinitely.

Purpose: The app currently uses `google/gemini-3-flash-preview` (unstable preview) for all 4 model slots with zero fallback. Title and summary generation have no timeout, no retry, and no circuit breaker. A model outage or slow response hangs the entire system.

Output: Stable model IDs with automatic fallback, resilience-wrapped title/summary generation with 10s timeouts, and streamText timeout configuration preventing Vercel 504s.
</objective>

<execution_context>
@/home/qualia/.claude/get-shit-done/workflows/execute-plan.md
@/home/qualia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-model-resilience-tool-hardening/16-RESEARCH.md

@lib/ai/providers.ts
@lib/resilience.ts
@app/(chat)/actions.ts
@lib/ai/conversation-summarizer.ts
@app/(chat)/api/chat/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pin stable model IDs and add OpenRouter fallback chain</name>
  <files>lib/ai/providers.ts</files>
  <action>
Replace ALL four model slots in `providers.ts` from `google/gemini-3-flash-preview` to `google/gemini-2.5-flash` (stable, GA since June 2025).

Add OpenRouter's native `models` array fallback via the `extraBody` option on each model definition. The fallback chain should be:
- Primary: `google/gemini-2.5-flash` (stable production model)
- Fallback: `google/gemini-2.5-flash-lite` (lighter variant, same family)

Apply this pattern to all four model slots: `chat-model`, `chat-model-reasoning`, `title-model`, `artifact-model`.

Use the `extraBody` parameter on each `openrouter()` call:
```typescript
openrouter("google/gemini-2.5-flash", {
  extraBody: {
    models: [
      "google/gemini-2.5-flash",
      "google/gemini-2.5-flash-lite",
    ],
  },
})
```

Update the comment from "Gemini 3 Flash Preview" to "Gemini 2.5 Flash (stable)" on each slot. Keep the test environment mock provider unchanged.
  </action>
  <verify>
Run `grep -c "gemini-3-flash-preview" lib/ai/providers.ts` -- must return 0 (no preview models left).
Run `grep -c "gemini-2.5-flash" lib/ai/providers.ts` -- must return 8+ (4 primary + 4 fallback references).
Run `grep -c "extraBody" lib/ai/providers.ts` -- must return 4 (one per model slot).
Run `pnpm lint` -- must pass.
  </verify>
  <done>All model slots use `google/gemini-2.5-flash` with `google/gemini-2.5-flash-lite` fallback. No preview model IDs remain.</done>
</task>

<task type="auto">
  <name>Task 2: Wrap title and summary generation in resilience + timeout</name>
  <files>app/(chat)/actions.ts, lib/ai/conversation-summarizer.ts</files>
  <action>
**In `app/(chat)/actions.ts`** -- `generateTitleFromUserMessage`:

1. Import `withAIGatewayResilience` from `@/lib/resilience`.
2. Wrap the `generateText` call inside `withAIGatewayResilience(async () => { ... })`.
3. Add `timeout: { totalMs: 10_000 }` to the `generateText` options (AI SDK native timeout, 10 second hard limit).
4. Wrap the entire function body in a try/catch. On ANY error (timeout, circuit breaker open, model failure), `console.warn` the error and return the fallback string `"New conversation"`. This matches the placeholder title already used in `route.ts` line 218.

The function should look like:
```typescript
export async function generateTitleFromUserMessage({ message }: { message: UIMessage }) {
  try {
    return await withAIGatewayResilience(async () => {
      const { text: title } = await generateText({
        model: myProvider.languageModel("title-model"),
        system: `...existing prompt...`,
        prompt: JSON.stringify(message),
        timeout: { totalMs: 10_000 },
      });
      return title;
    });
  } catch (error) {
    console.warn("Title generation failed, using fallback:", error);
    return "New conversation";
  }
}
```

**In `lib/ai/conversation-summarizer.ts`** -- `generateConversationSummary`:

1. Import `withAIGatewayResilience` from `@/lib/resilience`.
2. Move the `generateText` call inside `withAIGatewayResilience(async () => { ... })` within the existing try/catch.
3. Add `timeout: { totalMs: 10_000 }` to the `generateText` options.
4. The existing try/catch already returns `null` on failure -- keep that behavior. Just ensure `withAIGatewayResilience` is inside the try/catch so circuit breaker errors also get caught.
  </action>
  <verify>
Run `grep -c "withAIGatewayResilience" app/\(chat\)/actions.ts` -- must return 1.
Run `grep -c "withAIGatewayResilience" lib/ai/conversation-summarizer.ts` -- must return 1.
Run `grep -c "totalMs: 10_000" app/\(chat\)/actions.ts lib/ai/conversation-summarizer.ts` -- must return 2 (one per file).
Run `grep -c "New conversation" app/\(chat\)/actions.ts` -- must return 1 (the fallback).
Run `pnpm lint` -- must pass.
  </verify>
  <done>Title generation wrapped in withAIGatewayResilience + 10s timeout with "New conversation" fallback. Summary generation wrapped in withAIGatewayResilience + 10s timeout with null fallback.</done>
</task>

<task type="auto">
  <name>Task 3: Add timeout to main streamText call</name>
  <files>app/(chat)/api/chat/route.ts</files>
  <action>
In `app/(chat)/api/chat/route.ts`, add the `timeout` parameter to the `streamText` call (around line 330).

Add these two options to the `streamText` configuration object:
```typescript
timeout: {
  totalMs: 55_000,  // Just under Vercel's 60s limit (maxDuration = 60)
  chunkMs: 15_000,  // Abort if no chunk received for 15s (stalled stream detection)
},
abortSignal: request.signal,  // Propagate client disconnect to abort the stream
```

Place `timeout` and `abortSignal` alongside the existing `model`, `system`, `messages` parameters. The `request.signal` is already available from the `request: Request` parameter in the `POST` handler.

Do NOT add a manual AbortController -- the AI SDK `timeout` parameter handles it internally. Do NOT change `maxDuration` (it's already 60).
  </action>
  <verify>
Run `grep -c "totalMs: 55_000" app/\(chat\)/api/chat/route.ts` -- must return 1.
Run `grep -c "chunkMs: 15_000" app/\(chat\)/api/chat/route.ts` -- must return 1.
Run `grep -c "abortSignal: request.signal" app/\(chat\)/api/chat/route.ts` -- must return 1.
Run `pnpm lint` -- must pass.
  </verify>
  <done>streamText has 55s total timeout, 15s chunk stall detection, and client disconnect propagation. No Vercel 504 risk from hanging streams.</done>
</task>

</tasks>

<verification>
Requirements covered:
- RESIL-01: OpenRouter `models` array fallback on all 4 model slots
- RESIL-02: `google/gemini-2.5-flash` stable ID replaces `google/gemini-3-flash-preview`
- RESIL-03: `generateTitleFromUserMessage` wrapped in withAIGatewayResilience + 10s timeout
- RESIL-04: `streamText` has timeout (55s total, 15s chunk) + abortSignal
- RESIL-05: `generateConversationSummary` wrapped in withAIGatewayResilience + 10s timeout

All resilience patterns use existing infrastructure (lib/resilience.ts, AI SDK timeout). No new dependencies.
</verification>

<success_criteria>
1. `providers.ts` contains only `google/gemini-2.5-flash` model IDs (no preview slugs)
2. Each model slot has `extraBody.models` with primary + fallback
3. Title generation has 10s timeout and returns "New conversation" on failure
4. Summary generation has 10s timeout and returns null on failure
5. Main streamText has 55s total + 15s chunk timeout + client abort signal
6. `pnpm lint` passes
</success_criteria>

<output>
After completion, create `.planning/phases/16-model-resilience-tool-hardening/16-01-SUMMARY.md`
</output>
