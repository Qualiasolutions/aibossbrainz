# Milestone v1.3: AI Production Hardening

**Status:** SHIPPED 2026-02-18
**Phases:** 16-20
**Total Plans:** 10

## Overview

v1.3 remediates all critical and high-severity findings from the AI Production Audit (score 58/100, grade F). The milestone hardens model resilience and tool reliability first (the app breaks without these), then locks down security surfaces, adds safety rails for AI output, fixes the voice subsystem, and finishes with observability and cost controls that benefit from all other fixes being in place. 31 requirements across 5 phases.

## Phases

### Phase 16: Model Resilience & Tool Hardening

**Goal**: AI chat survives model outages gracefully and all tool invocations fail safely with user-friendly errors
**Depends on**: Nothing (first phase of v1.3)
**Plans**: 2 plans

Plans:

- [x] 16-01-PLAN.md — Model resilience: stable IDs, fallback chain, title/summary resilience wrappers, streamText timeout
- [x] 16-02-PLAN.md — Tool hardening: weather API error handling, requestSuggestions/strategyCanvas auth checks

**Details:**
- All 4 model slots pinned to stable `google/gemini-2.5-flash` with `google/gemini-2.5-flash-lite` fallback via OpenRouter's native `extraBody.models` array
- Title and summary generation wrapped in `withAIGatewayResilience` (circuit breaker + retry) with 10s hard timeout
- Main `streamText` call hardened with 55s total timeout, 15s chunk stall detection, and client disconnect propagation
- Weather tool: 10s fetch timeout, 5s geocoding timeout, response validation, user-friendly errors
- requestSuggestions: auth + ownership check with uniform error (no existence leak)
- strategyCanvas: ChatSDKError-specific error differentiation for better log triage

### Phase 17: Security Hardening

**Goal**: Known XSS vectors, auth bypasses, and information leaks in middleware and endpoints are closed
**Depends on**: Phase 16
**Plans**: 2 plans

Plans:

- [x] 17-01-PLAN.md — XSS removal (next/script) and middleware API route allowlist
- [x] 17-02-PLAN.md — Realtime Zod validation and health endpoint two-tier response

**Details:**
- Replaced dangerouslySetInnerHTML XSS vector with next/script (beforeInteractive strategy)
- Middleware API route allowlist: 6 public routes, all others require auth by default
- Both realtime routes validated with Zod schemas (5000-char message limit, enum botType, UUID chatId)
- Health endpoint: two-tier response (minimal for unauth, full details for auth users)

### Phase 18: Safety Rails

**Goal**: AI responses are filtered for safety, user PII is redacted before storage, and edge cases handled gracefully
**Depends on**: Phase 16 (model must be resilient before adding output filtering on top)
**Plans**: 2 plans

Plans:

- [x] 18-01-PLAN.md — PII redaction infrastructure, canary leak detection, safety middleware, message storage redaction, document prompt sanitization
- [x] 18-02-PLAN.md — Truncation detection with UI indicator, human escalation in system prompt, suggestion content validation

**Details:**
- PII redactor with Luhn-validated credit card, SSN, email, phone detection
- Canary token embedded in system prompts for leak detection
- AI SDK safety middleware on all 4 production model slots
- User messages PII-scrubbed before Postgres insert; streaming output scanned post-hoc
- Document prompt content sanitized with XML delimiters to prevent prompt injection
- Truncation detection with amber continue-banner (finishReason === 'length')
- Human escalation instructions in all three executive persona prompts
- Suggestion validation: 500/500/200 char limits + PII redaction

### Phase 19: Voice Quality

**Goal**: Voice playback produces clean audio with correct personas, optimized latency, and proper browser compatibility
**Depends on**: Nothing (self-contained voice subsystem)
**Plans**: 2 plans

Plans:

- [x] 19-01-PLAN.md — Config drift fix, shared markdown stripping, optimize_streaming_latency across all routes
- [x] 19-02-PLAN.md — Request stitching for collaborative audio quality, streaming endpoint for collab segments, user gesture gate for greeting

**Details:**
- Realtime route uses getVoiceConfig(botType) instead of hardcoded model/settings
- Shared stripMarkdownForTTS replaces inline regex chain in realtime route
- optimize_streaming_latency: 2 on all four ElevenLabs API calls (level 2 to preserve text normalization)
- Request stitching via previous_request_ids for prosody-aligned collaborative audio
- Sequential generation for collaborative segments (ordered request IDs required)
- Greeting audio gated behind user gesture (click/keydown) for autoplay compliance

### Phase 20: Observability & Cost Controls

**Goal**: Application has structured logging throughout, AI usage tracked with cost data, and spend alerts prevent bill shock
**Depends on**: Phases 16-19 (logging migration touches files modified by all prior phases)
**Plans**: 2 plans

Plans:

- [x] 20-01-PLAN.md — Cost infrastructure (AICostLog table, cost tracker, chat route cost recording, Stripe webhook logging migration, cost cron, cost dashboard API)
- [x] 20-02-PLAN.md — Broad structured logging migration across all remaining server-side files (98% achieved)

**Details:**
- AICostLog table with per-request cost tracking (separate table for granularity)
- Cost tracker: recordAICost, getDailyAICostTotal, getMonthlyCostSummary
- Chat route logs inputTokens, outputTokens, modelId, costUSD via after() (non-blocking)
- Stripe webhook: 33 console.* calls migrated to structured pino logging with request IDs
- Daily cost cron at 23:00 UTC with configurable threshold ($10 default)
- Monthly cost dashboard API (admin-only, per-model breakdown)
- 47 server-side files migrated to structured logging (98% coverage, 247/250 calls)

---

## Milestone Summary

**Key Decisions:**

- Gemini 2.5 Flash (stable GA) over Gemini 3 Flash Preview for production reliability
- OpenRouter extraBody.models for native fallback (no app-level retry needed)
- LanguageModelV2Middleware (AI SDK v5 type) for safety middleware
- Canary prefix match for partial leak detection
- Credit card Luhn + format check to reduce false positives
- Post-hoc streaming scan is detection/logging only (cannot recall streamed content)
- Truncation banner after Messages component (avoids prop drilling into memoized component)
- optimize_streaming_latency level 2 (not 3/4) to avoid text normalization issues
- Sequential generation for collaborative segments (request stitching requires ordered IDs)
- AICostLog as separate table for per-request granularity
- Non-blocking cost recording via after()
- Client-side files intentionally left with console.* (pino is server-only)

**Issues Resolved:**

- AI Production Audit score 58/100 (grade F) with 10 critical and 24 high findings — all addressed
- XSS vector via dangerouslySetInnerHTML eliminated
- Blanket /api/ middleware bypass replaced with allowlist
- Model instability from preview slugs fixed with stable versioned IDs
- Voice config drift between routes resolved with centralized voice-config.ts
- Collaborative audio pops/clicks fixed with request stitching
- Browser autoplay violations fixed with user gesture gate
- Unstructured logging (250 console.* calls) migrated to pino (98% coverage)

**Issues Deferred:**

- Monthly cost dashboard has no frontend consumer (API-only)
- AICostLog migration requires manual application via Supabase Dashboard
- Post-hoc streaming PII scan cannot recall already-streamed content (architectural limitation)
- 5 human verification scenarios pending (PII redaction, truncation UX, escalation, canary, suggestions)
- Collaborative audio quality needs human listening test
- 28 medium + 25 low severity findings deferred to v1.4

**Technical Debt Incurred:**

- Client-side console.* preserved in lib/utils.ts, lib/audio-manager.ts (pino is server-only)
- pnpm build fails locally without env vars (not a code issue, deploy works)

---

_For current project status, see .planning/MILESTONES.md_

---
